# configs file for wandb.config
---
model_name:
  desc: Name of variation of OPT from huggingface to initialise
  value: "facebook/opt-1.3b"
load_from_checkpoint:
  desc: Checkpoint path to load the model from, if None will instantiate from default
  value: null
  #value: "training_checkpoints/soft-opt-epoch=000-val_loss=21.112.ckpt"

# training config
# since we are using streaming dataloaders, we cannot define an epoch as end of the dataset, hence we use
# num_batches_per_epoch = steps_per_epoch / batch_size
max_epochs:
  desc: Maximum number of training epochs
  value: 30
steps_per_epoch:
  desc: Number of steps per epoch
  value: 8000
batch_size:
  desc: Batch size that each datamodule will output
  value: 32
check_val_every_n_epoch:
  desc: Every n epochs validation loop is run
  value: 2

# checkpoint configs
checkpoint_every_n_epochs:
  desc: Every n epochs we save a checkpoint of the model
  value: 30
checkpoint_save_dir:
  desc: Directory to save checkpoints to
  value: "training_checkpoints/14-06-2022-fine-tune/"

# optimizers config
optimizer_params:
  desc: Parameters for the optimizer in the form of a dictionary, same format as state_dict.
  # btw when using scientific notation write 1.0e-3 and not 1e-3 otherwise it is mistakenly parsed as string (bug on pyYAML side)
  value: {"lr": 1.0e-3}
lr_scheduler_params:
  desc: Parameters for the learning rate scheduler in the form of a dictionary, same format as state_dict.
  value: {"mode": "min", "patience": 10}
lr_scheduler_config:
  desc: Learning rate scheduler configuration for pytorch lightning
  value: {"monitor": "train_loss"}
...