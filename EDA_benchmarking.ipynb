{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce5f4e17-2400-4b69-8933-e3234d70b104",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5777276a-0e78-4151-8fa0-c73f18c37af6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weipy\\AppData\\Local\\Temp\\ipykernel_1084\\3854627690.py:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df111.mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": "bartscore             -2.157947\nbleuscore              0.342787\nrouge1_fmeasure        0.835004\nrouge1_precision       0.856809\nrouge1_recall          0.838207\nrouge2_fmeasure        0.737537\nrouge2_precision       0.756071\nrouge2_recall          0.743406\nrougeL_fmeasure        0.830460\nrougeL_precision       0.852231\nrougeL_recall          0.833340\nrougeLsum_fmeasure     0.830814\nrougeLsum_precision    0.852436\nrougeLsum_recall       0.833801\ndtype: float64"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = r\"metrics/benchmark_runs/model_benchmarked_results/1.3b-optimized-tokens=111-samples=500.pkl\"\n",
    "df111 = pd.read_pickle(filepath)\n",
    "df111.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a785aa1-1893-4a5b-bba6-781e284cea48",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weipy\\AppData\\Local\\Temp\\ipykernel_1084\\3997160809.py:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df163.mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": "bartscore             -2.193970\nbleuscore              0.316936\nrouge1_fmeasure        0.834778\nrouge1_precision       0.850439\nrouge1_recall          0.833884\nrouge2_fmeasure        0.721758\nrouge2_precision       0.734675\nrouge2_recall          0.722555\nrougeL_fmeasure        0.829546\nrougeL_precision       0.845049\nrougeL_recall          0.828588\nrougeLsum_fmeasure     0.829990\nrougeLsum_precision    0.845449\nrougeLsum_recall       0.829088\ndtype: float64"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = r\"metrics/benchmark_runs/model_benchmarked_results/1.3b-optimized-tokens=163-samples=500.pkl\"\n",
    "df163 = pd.read_pickle(filepath)\n",
    "df163.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "275fe94e-476a-436b-ac24-1c5eefac27c7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weipy\\AppData\\Local\\Temp\\ipykernel_1084\\525108569.py:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_finetuned.mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": "bartscore             -4.325089\nbleuscore              0.025170\nrouge1_fmeasure        0.315754\nrouge1_precision       0.304833\nrouge1_recall          0.374748\nrouge2_fmeasure        0.140419\nrouge2_precision       0.130611\nrouge2_recall          0.178818\nrougeL_fmeasure        0.300252\nrougeL_precision       0.288716\nrougeL_recall          0.358478\nrougeLsum_fmeasure     0.302298\nrougeLsum_precision    0.290669\nrougeLsum_recall       0.360918\ndtype: float64"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = r\"metrics/benchmark_runs/model_benchmarked_results/1.3b-fine-tuned-samples=500.pkl\"\n",
    "df_finetuned = pd.read_pickle(filepath)\n",
    "df_finetuned.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weipy\\AppData\\Local\\Temp\\ipykernel_1084\\3846594080.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_bart.mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": "bartscore             -2.657477\nbleuscore              0.083362\nrouge1_fmeasure        0.316741\nrouge1_precision       0.207854\nrouge1_recall          0.935199\nrouge2_fmeasure        0.251569\nrouge2_precision       0.164845\nrouge2_recall          0.816269\nrougeL_fmeasure        0.301592\nrougeL_precision       0.197495\nrougeL_recall          0.900656\nrougeLsum_fmeasure     0.309371\nrougeLsum_precision    0.202609\nrougeLsum_recall       0.920124\ndtype: float64"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bart = pd.read_pickle(r\"metrics/benchmark_runs/model_benchmarked_results/bart-samples=500.pkl\")\n",
    "df_bart.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weipy\\AppData\\Local\\Temp\\ipykernel_1084\\853246069.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df20.mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": "bartscore             -3.025109\nbleuscore              0.246091\nrouge1_fmeasure        0.632655\nrouge1_precision       0.700080\nrouge1_recall          0.636459\nrouge2_fmeasure        0.538138\nrouge2_precision       0.590409\nrouge2_recall          0.540979\nrougeL_fmeasure        0.626995\nrougeL_precision       0.693667\nrougeL_recall          0.630616\nrougeLsum_fmeasure     0.626495\nrougeLsum_precision    0.693297\nrougeLsum_recall       0.629847\ndtype: float64"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df20 = pd.read_pickle(r\"metrics/benchmark_runs/model_benchmarked_results/1.3b-paracombined-epoch=269-samples=100.pkl\")\n",
    "df20.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weipy\\AppData\\Local\\Temp\\ipykernel_41852\\746301827.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df143.mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": "bartscore             -1.682499\nbleuscore              0.481043\nrouge1_fmeasure        0.832861\nrouge1_precision       0.848814\nrouge1_recall          0.837896\nrouge2_fmeasure        0.716814\nrouge2_precision       0.730555\nrouge2_recall          0.722471\nrougeL_fmeasure        0.827449\nrougeL_precision       0.843178\nrougeL_recall          0.832482\nrougeLsum_fmeasure     0.827528\nrougeLsum_precision    0.843265\nrougeLsum_recall       0.832525\ndtype: float64"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df143 = pd.read_pickle(r\"metrics/benchmark_runs/model_benchmarked_results/optim-tokens=143.pkl\")\n",
    "df143.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   src  \\\n0    david schwartz was unaware of the fact that he...   \n1                             we 'll be at the house .   \n2    why do n't you come in and we 'll talk about it .   \n3                                  What was your plan?   \n4    historians can make a credible argument that t...   \n..                                                 ...   \n495  adler took a moment to consider the situation ...   \n496                                      Five minutes.   \n497                          i , uh -- i brought you .   \n498                          Every child is different.   \n499                        Qatar motorcycle Grand Prix   \n\n                                                target  bartscore  bleuscore  \\\n0    david schwartz was unaware of how narrowly he ...  -2.717181   0.214486   \n1                                 we'il be safe here .  -5.165600   0.000000   \n2    why do n't you come in and we 'll talk about it .  -0.299599   1.000000   \n3                                  What was your plan?  -0.602847   1.000000   \n4    historians can make a credible case that perio...  -1.878352   0.482163   \n..                                                 ...        ...        ...   \n495  adler took a few seconds to consider that , th...  -2.556202   0.000000   \n496                Ladies and gentlemen, five minutes.  -2.346147   0.000000   \n497                    i , uh -- i brought you , uh --  -1.648143   0.654891   \n498                          Every child is different.  -0.453974   1.000000   \n499                        Qatar motorcycle Grand Prix  -1.097775   1.000000   \n\n     rouge1_fmeasure  rouge1_precision  rouge1_recall  rouge2_fmeasure  \\\n0           0.558140          0.480000       0.666667         0.341463   \n1           0.363636          0.333333       0.400000         0.000000   \n2           1.000000          1.000000       1.000000         1.000000   \n3           1.000000          1.000000       1.000000         1.000000   \n4           0.777778          0.777778       0.777778         0.653846   \n..               ...               ...            ...              ...   \n495         0.666667          0.700000       0.636364         0.421053   \n496         0.571429          1.000000       0.400000         0.400000   \n497         0.909091          1.000000       0.833333         0.888889   \n498         1.000000          1.000000       1.000000         1.000000   \n499         1.000000          1.000000       1.000000         1.000000   \n\n     rouge2_precision  rouge2_recall  rougeL_fmeasure  rougeL_precision  \\\n0            0.291667       0.411765         0.511628          0.440000   \n1            0.000000       0.000000         0.363636          0.333333   \n2            1.000000       1.000000         1.000000          1.000000   \n3            1.000000       1.000000         1.000000          1.000000   \n4            0.653846       0.653846         0.777778          0.777778   \n..                ...            ...              ...               ...   \n495          0.444444       0.400000         0.666667          0.700000   \n496          1.000000       0.250000         0.571429          1.000000   \n497          1.000000       0.800000         0.909091          1.000000   \n498          1.000000       1.000000         1.000000          1.000000   \n499          1.000000       1.000000         1.000000          1.000000   \n\n     rougeL_recall  rougeLsum_fmeasure  rougeLsum_precision  rougeLsum_recall  \n0         0.611111            0.511628             0.440000          0.611111  \n1         0.400000            0.363636             0.333333          0.400000  \n2         1.000000            1.000000             1.000000          1.000000  \n3         1.000000            1.000000             1.000000          1.000000  \n4         0.777778            0.777778             0.777778          0.777778  \n..             ...                 ...                  ...               ...  \n495       0.636364            0.666667             0.700000          0.636364  \n496       0.400000            0.571429             1.000000          0.400000  \n497       0.833333            0.909091             1.000000          0.833333  \n498       1.000000            1.000000             1.000000          1.000000  \n499       1.000000            1.000000             1.000000          1.000000  \n\n[500 rows x 16 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>src</th>\n      <th>target</th>\n      <th>bartscore</th>\n      <th>bleuscore</th>\n      <th>rouge1_fmeasure</th>\n      <th>rouge1_precision</th>\n      <th>rouge1_recall</th>\n      <th>rouge2_fmeasure</th>\n      <th>rouge2_precision</th>\n      <th>rouge2_recall</th>\n      <th>rougeL_fmeasure</th>\n      <th>rougeL_precision</th>\n      <th>rougeL_recall</th>\n      <th>rougeLsum_fmeasure</th>\n      <th>rougeLsum_precision</th>\n      <th>rougeLsum_recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>david schwartz was unaware of the fact that he...</td>\n      <td>david schwartz was unaware of how narrowly he ...</td>\n      <td>-2.717181</td>\n      <td>0.214486</td>\n      <td>0.558140</td>\n      <td>0.480000</td>\n      <td>0.666667</td>\n      <td>0.341463</td>\n      <td>0.291667</td>\n      <td>0.411765</td>\n      <td>0.511628</td>\n      <td>0.440000</td>\n      <td>0.611111</td>\n      <td>0.511628</td>\n      <td>0.440000</td>\n      <td>0.611111</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>we 'll be at the house .</td>\n      <td>we'il be safe here .</td>\n      <td>-5.165600</td>\n      <td>0.000000</td>\n      <td>0.363636</td>\n      <td>0.333333</td>\n      <td>0.400000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.363636</td>\n      <td>0.333333</td>\n      <td>0.400000</td>\n      <td>0.363636</td>\n      <td>0.333333</td>\n      <td>0.400000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>why do n't you come in and we 'll talk about it .</td>\n      <td>why do n't you come in and we 'll talk about it .</td>\n      <td>-0.299599</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>What was your plan?</td>\n      <td>What was your plan?</td>\n      <td>-0.602847</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>historians can make a credible argument that t...</td>\n      <td>historians can make a credible case that perio...</td>\n      <td>-1.878352</td>\n      <td>0.482163</td>\n      <td>0.777778</td>\n      <td>0.777778</td>\n      <td>0.777778</td>\n      <td>0.653846</td>\n      <td>0.653846</td>\n      <td>0.653846</td>\n      <td>0.777778</td>\n      <td>0.777778</td>\n      <td>0.777778</td>\n      <td>0.777778</td>\n      <td>0.777778</td>\n      <td>0.777778</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>495</th>\n      <td>adler took a moment to consider the situation ...</td>\n      <td>adler took a few seconds to consider that , th...</td>\n      <td>-2.556202</td>\n      <td>0.000000</td>\n      <td>0.666667</td>\n      <td>0.700000</td>\n      <td>0.636364</td>\n      <td>0.421053</td>\n      <td>0.444444</td>\n      <td>0.400000</td>\n      <td>0.666667</td>\n      <td>0.700000</td>\n      <td>0.636364</td>\n      <td>0.666667</td>\n      <td>0.700000</td>\n      <td>0.636364</td>\n    </tr>\n    <tr>\n      <th>496</th>\n      <td>Five minutes.</td>\n      <td>Ladies and gentlemen, five minutes.</td>\n      <td>-2.346147</td>\n      <td>0.000000</td>\n      <td>0.571429</td>\n      <td>1.000000</td>\n      <td>0.400000</td>\n      <td>0.400000</td>\n      <td>1.000000</td>\n      <td>0.250000</td>\n      <td>0.571429</td>\n      <td>1.000000</td>\n      <td>0.400000</td>\n      <td>0.571429</td>\n      <td>1.000000</td>\n      <td>0.400000</td>\n    </tr>\n    <tr>\n      <th>497</th>\n      <td>i , uh -- i brought you .</td>\n      <td>i , uh -- i brought you , uh --</td>\n      <td>-1.648143</td>\n      <td>0.654891</td>\n      <td>0.909091</td>\n      <td>1.000000</td>\n      <td>0.833333</td>\n      <td>0.888889</td>\n      <td>1.000000</td>\n      <td>0.800000</td>\n      <td>0.909091</td>\n      <td>1.000000</td>\n      <td>0.833333</td>\n      <td>0.909091</td>\n      <td>1.000000</td>\n      <td>0.833333</td>\n    </tr>\n    <tr>\n      <th>498</th>\n      <td>Every child is different.</td>\n      <td>Every child is different.</td>\n      <td>-0.453974</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>499</th>\n      <td>Qatar motorcycle Grand Prix</td>\n      <td>Qatar motorcycle Grand Prix</td>\n      <td>-1.097775</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>500 rows × 16 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df143"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weipy\\AppData\\Local\\Temp\\ipykernel_1084\\1188306197.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_res = pd.concat([df20.mean(), df111.mean(), df163.mean(), df_finetuned.mean(), df_bart.mean()], keys=['soft prompt 20 tokens', 'soft prompt 111 tokens', 'soft prompt 163 tokens', 'fine tuned', 'bart fine tuned'], axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                     soft prompt 20 tokens  soft prompt 111 tokens  \\\nbartscore                        -3.025109               -2.157947   \nbleuscore                         0.246091                0.342787   \nrouge1_fmeasure                   0.632655                0.835004   \nrouge1_precision                  0.700080                0.856809   \nrouge1_recall                     0.636459                0.838207   \nrouge2_fmeasure                   0.538138                0.737537   \nrouge2_precision                  0.590409                0.756071   \nrouge2_recall                     0.540979                0.743406   \nrougeL_fmeasure                   0.626995                0.830460   \nrougeL_precision                  0.693667                0.852231   \nrougeL_recall                     0.630616                0.833340   \nrougeLsum_fmeasure                0.626495                0.830814   \nrougeLsum_precision               0.693297                0.852436   \nrougeLsum_recall                  0.629847                0.833801   \n\n                     soft prompt 163 tokens  fine tuned  bart fine tuned  \nbartscore                         -2.193970   -4.325089        -2.657477  \nbleuscore                          0.316936    0.025170         0.083362  \nrouge1_fmeasure                    0.834778    0.315754         0.316741  \nrouge1_precision                   0.850439    0.304833         0.207854  \nrouge1_recall                      0.833884    0.374748         0.935199  \nrouge2_fmeasure                    0.721758    0.140419         0.251569  \nrouge2_precision                   0.734675    0.130611         0.164845  \nrouge2_recall                      0.722555    0.178818         0.816269  \nrougeL_fmeasure                    0.829546    0.300252         0.301592  \nrougeL_precision                   0.845049    0.288716         0.197495  \nrougeL_recall                      0.828588    0.358478         0.900656  \nrougeLsum_fmeasure                 0.829990    0.302298         0.309371  \nrougeLsum_precision                0.845449    0.290669         0.202609  \nrougeLsum_recall                   0.829088    0.360918         0.920124  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>soft prompt 20 tokens</th>\n      <th>soft prompt 111 tokens</th>\n      <th>soft prompt 163 tokens</th>\n      <th>fine tuned</th>\n      <th>bart fine tuned</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>bartscore</th>\n      <td>-3.025109</td>\n      <td>-2.157947</td>\n      <td>-2.193970</td>\n      <td>-4.325089</td>\n      <td>-2.657477</td>\n    </tr>\n    <tr>\n      <th>bleuscore</th>\n      <td>0.246091</td>\n      <td>0.342787</td>\n      <td>0.316936</td>\n      <td>0.025170</td>\n      <td>0.083362</td>\n    </tr>\n    <tr>\n      <th>rouge1_fmeasure</th>\n      <td>0.632655</td>\n      <td>0.835004</td>\n      <td>0.834778</td>\n      <td>0.315754</td>\n      <td>0.316741</td>\n    </tr>\n    <tr>\n      <th>rouge1_precision</th>\n      <td>0.700080</td>\n      <td>0.856809</td>\n      <td>0.850439</td>\n      <td>0.304833</td>\n      <td>0.207854</td>\n    </tr>\n    <tr>\n      <th>rouge1_recall</th>\n      <td>0.636459</td>\n      <td>0.838207</td>\n      <td>0.833884</td>\n      <td>0.374748</td>\n      <td>0.935199</td>\n    </tr>\n    <tr>\n      <th>rouge2_fmeasure</th>\n      <td>0.538138</td>\n      <td>0.737537</td>\n      <td>0.721758</td>\n      <td>0.140419</td>\n      <td>0.251569</td>\n    </tr>\n    <tr>\n      <th>rouge2_precision</th>\n      <td>0.590409</td>\n      <td>0.756071</td>\n      <td>0.734675</td>\n      <td>0.130611</td>\n      <td>0.164845</td>\n    </tr>\n    <tr>\n      <th>rouge2_recall</th>\n      <td>0.540979</td>\n      <td>0.743406</td>\n      <td>0.722555</td>\n      <td>0.178818</td>\n      <td>0.816269</td>\n    </tr>\n    <tr>\n      <th>rougeL_fmeasure</th>\n      <td>0.626995</td>\n      <td>0.830460</td>\n      <td>0.829546</td>\n      <td>0.300252</td>\n      <td>0.301592</td>\n    </tr>\n    <tr>\n      <th>rougeL_precision</th>\n      <td>0.693667</td>\n      <td>0.852231</td>\n      <td>0.845049</td>\n      <td>0.288716</td>\n      <td>0.197495</td>\n    </tr>\n    <tr>\n      <th>rougeL_recall</th>\n      <td>0.630616</td>\n      <td>0.833340</td>\n      <td>0.828588</td>\n      <td>0.358478</td>\n      <td>0.900656</td>\n    </tr>\n    <tr>\n      <th>rougeLsum_fmeasure</th>\n      <td>0.626495</td>\n      <td>0.830814</td>\n      <td>0.829990</td>\n      <td>0.302298</td>\n      <td>0.309371</td>\n    </tr>\n    <tr>\n      <th>rougeLsum_precision</th>\n      <td>0.693297</td>\n      <td>0.852436</td>\n      <td>0.845449</td>\n      <td>0.290669</td>\n      <td>0.202609</td>\n    </tr>\n    <tr>\n      <th>rougeLsum_recall</th>\n      <td>0.629847</td>\n      <td>0.833801</td>\n      <td>0.829088</td>\n      <td>0.360918</td>\n      <td>0.920124</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res = pd.concat([df20.mean(), df111.mean(), df163.mean(), df_finetuned.mean(), df_bart.mean()], keys=['soft prompt 20 tokens', 'soft prompt 111 tokens', 'soft prompt 163 tokens', 'fine tuned', 'bart fine tuned'], axis=1)\n",
    "df_res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                     |   soft prompt 20 tokens |   soft prompt 111 tokens |   soft prompt 163 tokens |   fine tuned |   bart fine tuned |\n",
      "|:--------------------|------------------------:|-------------------------:|-------------------------:|-------------:|------------------:|\n",
      "| bartscore           |               -3.02511  |                -2.15795  |                -2.19397  |   -4.32509   |        -2.65748   |\n",
      "| bleuscore           |                0.246091 |                 0.342787 |                 0.316936 |    0.0251696 |         0.0833621 |\n",
      "| rouge1_fmeasure     |                0.632655 |                 0.835004 |                 0.834778 |    0.315754  |         0.316741  |\n",
      "| rouge1_precision    |                0.70008  |                 0.856809 |                 0.850439 |    0.304833  |         0.207854  |\n",
      "| rouge1_recall       |                0.636459 |                 0.838207 |                 0.833884 |    0.374748  |         0.935199  |\n",
      "| rouge2_fmeasure     |                0.538138 |                 0.737537 |                 0.721758 |    0.140419  |         0.251569  |\n",
      "| rouge2_precision    |                0.590409 |                 0.756071 |                 0.734675 |    0.130611  |         0.164845  |\n",
      "| rouge2_recall       |                0.540979 |                 0.743406 |                 0.722555 |    0.178818  |         0.816269  |\n",
      "| rougeL_fmeasure     |                0.626995 |                 0.83046  |                 0.829546 |    0.300252  |         0.301592  |\n",
      "| rougeL_precision    |                0.693667 |                 0.852231 |                 0.845049 |    0.288716  |         0.197495  |\n",
      "| rougeL_recall       |                0.630616 |                 0.83334  |                 0.828588 |    0.358478  |         0.900656  |\n",
      "| rougeLsum_fmeasure  |                0.626495 |                 0.830814 |                 0.82999  |    0.302298  |         0.309371  |\n",
      "| rougeLsum_precision |                0.693297 |                 0.852436 |                 0.845449 |    0.290669  |         0.202609  |\n",
      "| rougeLsum_recall    |                0.629847 |                 0.833801 |                 0.829088 |    0.360918  |         0.920124  |\n"
     ]
    }
   ],
   "source": [
    "print(df_res.to_markdown())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|        | 14                     |\n",
      "|:-------|:-----------------------|\n",
      "| src    | His father killed him. |\n",
      "| target | His father killed him. |\n"
     ]
    }
   ],
   "source": [
    "print(df111.iloc[14, [0,1]].to_markdown())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   src  \\\n255  E-4171/10 (FR) Véronique De Keyser (S&D) to th...   \n132  P-007303/11 (PT) Luís Paulo Alves (S&D) to the...   \n443  E-002681/11 (EN) Proinsias De Rossa (S&D) to t...   \n78   E-7769/10 (FR) Vladko Todorov Panayotov (ALDE)...   \n151  On 1 May 2004, the Czech Republic, Estonia, Cy...   \n..                                                 ...   \n397  you 're n't n't n't n't n't n't n't n't n't n'...   \n425   i 'm the attorney general of the united states .   \n163  and the two of them are , uh , they ' re trave...   \n183  38 - opinion in case of the case of the case o...   \n72                                you 're a good guy .   \n\n                                                target  bartscore  bleuscore  \\\n255  E-4171/10 (FR) Véronique De Keyser (S&D) to th...  -0.272235   0.838577   \n132  P-007303/11 (PT) Luís Paulo Alves (S&D) to the...  -0.291804   0.722160   \n443  E-002681/11 (EN) Proinsias De Rossa (S&D) to t...  -0.299017   0.838577   \n78   E-7769/10 (FR) Vladko Todorov Panayotov (ALDE)...  -0.306259   0.912168   \n151  On 1 May 2004, the Czech Republic, Estonia, Cy...  -0.317503   0.766120   \n..                                                 ...        ...        ...   \n397      you wan na be friends with tess , do n't be .  -5.316466   0.000000   \n425  may we have the name and number of your attorn...  -5.580197   0.000000   \n163  i suppose tegger and warvia told you , they ’ ...  -5.642324   0.000000   \n183  38 – opinion in case c‑105/02 , paragraphs 85 ...  -5.900317   0.000000   \n72             you know , before the crotch dried up .  -6.000955   0.000000   \n\n     rouge1_fmeasure  rouge1_precision  rouge1_recall  rouge2_fmeasure  \\\n255         0.941176          0.888889       1.000000         0.937500   \n132         0.914286          0.842105       1.000000         0.909091   \n443         0.937500          0.882353       1.000000         0.933333   \n78          0.965517          0.933333       1.000000         0.962963   \n151         0.837209          0.818182       0.857143         0.780488   \n..               ...               ...            ...              ...   \n397         0.146341          0.100000       0.272727         0.051282   \n425         0.315789          0.333333       0.300000         0.000000   \n163         0.333333          0.333333       0.333333         0.090909   \n183         0.195122          0.133333       0.363636         0.153846   \n72          0.166667          0.200000       0.142857         0.000000   \n\n     rouge2_precision  rouge2_recall  rougeL_fmeasure  rougeL_precision  \\\n255          0.882353       1.000000         0.941176          0.888889   \n132          0.833333       1.000000         0.914286          0.842105   \n443          0.875000       1.000000         0.937500          0.882353   \n78           0.928571       1.000000         0.965517          0.933333   \n151          0.761905       0.800000         0.837209          0.818182   \n..                ...            ...              ...               ...   \n397          0.034483       0.100000         0.146341          0.100000   \n425          0.000000       0.000000         0.210526          0.222222   \n163          0.090909       0.090909         0.333333          0.333333   \n183          0.103448       0.300000         0.195122          0.133333   \n72           0.000000       0.000000         0.166667          0.200000   \n\n     rougeL_recall  rougeLsum_fmeasure  rougeLsum_precision  rougeLsum_recall  \n255       1.000000            0.941176             0.888889          1.000000  \n132       1.000000            0.914286             0.842105          1.000000  \n443       1.000000            0.937500             0.882353          1.000000  \n78        1.000000            0.965517             0.933333          1.000000  \n151       0.857143            0.837209             0.818182          0.857143  \n..             ...                 ...                  ...               ...  \n397       0.272727            0.146341             0.100000          0.272727  \n425       0.200000            0.210526             0.222222          0.200000  \n163       0.333333            0.333333             0.333333          0.333333  \n183       0.363636            0.195122             0.133333          0.363636  \n72        0.142857            0.166667             0.200000          0.142857  \n\n[299 rows x 16 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>src</th>\n      <th>target</th>\n      <th>bartscore</th>\n      <th>bleuscore</th>\n      <th>rouge1_fmeasure</th>\n      <th>rouge1_precision</th>\n      <th>rouge1_recall</th>\n      <th>rouge2_fmeasure</th>\n      <th>rouge2_precision</th>\n      <th>rouge2_recall</th>\n      <th>rougeL_fmeasure</th>\n      <th>rougeL_precision</th>\n      <th>rougeL_recall</th>\n      <th>rougeLsum_fmeasure</th>\n      <th>rougeLsum_precision</th>\n      <th>rougeLsum_recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>255</th>\n      <td>E-4171/10 (FR) Véronique De Keyser (S&amp;D) to th...</td>\n      <td>E-4171/10 (FR) Véronique De Keyser (S&amp;D) to th...</td>\n      <td>-0.272235</td>\n      <td>0.838577</td>\n      <td>0.941176</td>\n      <td>0.888889</td>\n      <td>1.000000</td>\n      <td>0.937500</td>\n      <td>0.882353</td>\n      <td>1.000000</td>\n      <td>0.941176</td>\n      <td>0.888889</td>\n      <td>1.000000</td>\n      <td>0.941176</td>\n      <td>0.888889</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>132</th>\n      <td>P-007303/11 (PT) Luís Paulo Alves (S&amp;D) to the...</td>\n      <td>P-007303/11 (PT) Luís Paulo Alves (S&amp;D) to the...</td>\n      <td>-0.291804</td>\n      <td>0.722160</td>\n      <td>0.914286</td>\n      <td>0.842105</td>\n      <td>1.000000</td>\n      <td>0.909091</td>\n      <td>0.833333</td>\n      <td>1.000000</td>\n      <td>0.914286</td>\n      <td>0.842105</td>\n      <td>1.000000</td>\n      <td>0.914286</td>\n      <td>0.842105</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>443</th>\n      <td>E-002681/11 (EN) Proinsias De Rossa (S&amp;D) to t...</td>\n      <td>E-002681/11 (EN) Proinsias De Rossa (S&amp;D) to t...</td>\n      <td>-0.299017</td>\n      <td>0.838577</td>\n      <td>0.937500</td>\n      <td>0.882353</td>\n      <td>1.000000</td>\n      <td>0.933333</td>\n      <td>0.875000</td>\n      <td>1.000000</td>\n      <td>0.937500</td>\n      <td>0.882353</td>\n      <td>1.000000</td>\n      <td>0.937500</td>\n      <td>0.882353</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>E-7769/10 (FR) Vladko Todorov Panayotov (ALDE)...</td>\n      <td>E-7769/10 (FR) Vladko Todorov Panayotov (ALDE)...</td>\n      <td>-0.306259</td>\n      <td>0.912168</td>\n      <td>0.965517</td>\n      <td>0.933333</td>\n      <td>1.000000</td>\n      <td>0.962963</td>\n      <td>0.928571</td>\n      <td>1.000000</td>\n      <td>0.965517</td>\n      <td>0.933333</td>\n      <td>1.000000</td>\n      <td>0.965517</td>\n      <td>0.933333</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>151</th>\n      <td>On 1 May 2004, the Czech Republic, Estonia, Cy...</td>\n      <td>On 1 May 2004, the Czech Republic, Estonia, Cy...</td>\n      <td>-0.317503</td>\n      <td>0.766120</td>\n      <td>0.837209</td>\n      <td>0.818182</td>\n      <td>0.857143</td>\n      <td>0.780488</td>\n      <td>0.761905</td>\n      <td>0.800000</td>\n      <td>0.837209</td>\n      <td>0.818182</td>\n      <td>0.857143</td>\n      <td>0.837209</td>\n      <td>0.818182</td>\n      <td>0.857143</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>397</th>\n      <td>you 're n't n't n't n't n't n't n't n't n't n'...</td>\n      <td>you wan na be friends with tess , do n't be .</td>\n      <td>-5.316466</td>\n      <td>0.000000</td>\n      <td>0.146341</td>\n      <td>0.100000</td>\n      <td>0.272727</td>\n      <td>0.051282</td>\n      <td>0.034483</td>\n      <td>0.100000</td>\n      <td>0.146341</td>\n      <td>0.100000</td>\n      <td>0.272727</td>\n      <td>0.146341</td>\n      <td>0.100000</td>\n      <td>0.272727</td>\n    </tr>\n    <tr>\n      <th>425</th>\n      <td>i 'm the attorney general of the united states .</td>\n      <td>may we have the name and number of your attorn...</td>\n      <td>-5.580197</td>\n      <td>0.000000</td>\n      <td>0.315789</td>\n      <td>0.333333</td>\n      <td>0.300000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.210526</td>\n      <td>0.222222</td>\n      <td>0.200000</td>\n      <td>0.210526</td>\n      <td>0.222222</td>\n      <td>0.200000</td>\n    </tr>\n    <tr>\n      <th>163</th>\n      <td>and the two of them are , uh , they ' re trave...</td>\n      <td>i suppose tegger and warvia told you , they ’ ...</td>\n      <td>-5.642324</td>\n      <td>0.000000</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <th>183</th>\n      <td>38 - opinion in case of the case of the case o...</td>\n      <td>38 – opinion in case c‑105/02 , paragraphs 85 ...</td>\n      <td>-5.900317</td>\n      <td>0.000000</td>\n      <td>0.195122</td>\n      <td>0.133333</td>\n      <td>0.363636</td>\n      <td>0.153846</td>\n      <td>0.103448</td>\n      <td>0.300000</td>\n      <td>0.195122</td>\n      <td>0.133333</td>\n      <td>0.363636</td>\n      <td>0.195122</td>\n      <td>0.133333</td>\n      <td>0.363636</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>you 're a good guy .</td>\n      <td>you know , before the crotch dried up .</td>\n      <td>-6.000955</td>\n      <td>0.000000</td>\n      <td>0.166667</td>\n      <td>0.200000</td>\n      <td>0.142857</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.166667</td>\n      <td>0.200000</td>\n      <td>0.142857</td>\n      <td>0.166667</td>\n      <td>0.200000</td>\n      <td>0.142857</td>\n    </tr>\n  </tbody>\n</table>\n<p>299 rows × 16 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_finetuned.sort_values('bartscore', ascending=False)\n",
    "df = df[df['src']!=df['target']]\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   src  \\\n295                Russia has not ratified the UNCPMF.   \n406                                     $ 60 million .   \n452  What's your relationship with the man who kill...   \n198                                  kris left rehab .   \n204  || ≥ 1 % of the total number of the child-resi...   \n..                                                 ...   \n306                          there 's something here !   \n309                                    Road transport.   \n312                                 Molecular biology.   \n188                       Economic and Monetary Union.   \n258                       Germany, Europe, and Russia.   \n\n                                                target  bartscore  bleuscore  \\\n295  Russia has not ratified the UN Convention on t...  -2.362773   0.052796   \n406                             Sixty million dollars.  -2.624635   0.000000   \n452       What is your relationship with Howard Stark?  -2.953407   0.000000   \n198                       kris just got out of rehab .  -2.264951   0.000000   \n204  || ≥ 1 % which are offered or sold to the gene...  -2.883121   0.356886   \n..                                                 ...        ...        ...   \n306                         there ’ s something here !  -2.263093   0.000000   \n309                                     Road transport  -2.669473   0.000000   \n312                                  Molecular biology  -2.083886   0.000000   \n188                        Economic and Monetary Union  -1.832057   0.000000   \n258                        Germany, Europe, and Russia  -1.551258   0.000000   \n\n     rouge1_fmeasure  rouge1_precision  rouge1_recall  rouge2_fmeasure  \\\n295         0.357143          0.833333       0.227273         0.307692   \n406         0.400000          0.500000       0.333333         0.000000   \n452         0.444444          0.363636       0.571429         0.250000   \n198         0.444444          0.666667       0.333333         0.000000   \n204         0.500000          0.500000       0.500000         0.352941   \n..               ...               ...            ...              ...   \n306         1.000000          1.000000       1.000000         1.000000   \n309         1.000000          1.000000       1.000000         1.000000   \n312         1.000000          1.000000       1.000000         1.000000   \n188         1.000000          1.000000       1.000000         1.000000   \n258         1.000000          1.000000       1.000000         1.000000   \n\n     rouge2_precision  rouge2_recall  rougeL_fmeasure  rougeL_precision  \\\n295          0.800000       0.190476         0.357143          0.833333   \n406          0.000000       0.000000         0.400000          0.500000   \n452          0.200000       0.333333         0.444444          0.363636   \n198          0.000000       0.000000         0.444444          0.666667   \n204          0.352941       0.352941         0.500000          0.500000   \n..                ...            ...              ...               ...   \n306          1.000000       1.000000         1.000000          1.000000   \n309          1.000000       1.000000         1.000000          1.000000   \n312          1.000000       1.000000         1.000000          1.000000   \n188          1.000000       1.000000         1.000000          1.000000   \n258          1.000000       1.000000         1.000000          1.000000   \n\n     rougeL_recall  rougeLsum_fmeasure  rougeLsum_precision  rougeLsum_recall  \n295       0.227273            0.357143             0.833333          0.227273  \n406       0.333333            0.400000             0.500000          0.333333  \n452       0.571429            0.444444             0.363636          0.571429  \n198       0.333333            0.444444             0.666667          0.333333  \n204       0.500000            0.500000             0.500000          0.500000  \n..             ...                 ...                  ...               ...  \n306       1.000000            1.000000             1.000000          1.000000  \n309       1.000000            1.000000             1.000000          1.000000  \n312       1.000000            1.000000             1.000000          1.000000  \n188       1.000000            1.000000             1.000000          1.000000  \n258       1.000000            1.000000             1.000000          1.000000  \n\n[145 rows x 16 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>src</th>\n      <th>target</th>\n      <th>bartscore</th>\n      <th>bleuscore</th>\n      <th>rouge1_fmeasure</th>\n      <th>rouge1_precision</th>\n      <th>rouge1_recall</th>\n      <th>rouge2_fmeasure</th>\n      <th>rouge2_precision</th>\n      <th>rouge2_recall</th>\n      <th>rougeL_fmeasure</th>\n      <th>rougeL_precision</th>\n      <th>rougeL_recall</th>\n      <th>rougeLsum_fmeasure</th>\n      <th>rougeLsum_precision</th>\n      <th>rougeLsum_recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>295</th>\n      <td>Russia has not ratified the UNCPMF.</td>\n      <td>Russia has not ratified the UN Convention on t...</td>\n      <td>-2.362773</td>\n      <td>0.052796</td>\n      <td>0.357143</td>\n      <td>0.833333</td>\n      <td>0.227273</td>\n      <td>0.307692</td>\n      <td>0.800000</td>\n      <td>0.190476</td>\n      <td>0.357143</td>\n      <td>0.833333</td>\n      <td>0.227273</td>\n      <td>0.357143</td>\n      <td>0.833333</td>\n      <td>0.227273</td>\n    </tr>\n    <tr>\n      <th>406</th>\n      <td>$ 60 million .</td>\n      <td>Sixty million dollars.</td>\n      <td>-2.624635</td>\n      <td>0.000000</td>\n      <td>0.400000</td>\n      <td>0.500000</td>\n      <td>0.333333</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.400000</td>\n      <td>0.500000</td>\n      <td>0.333333</td>\n      <td>0.400000</td>\n      <td>0.500000</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <th>452</th>\n      <td>What's your relationship with the man who kill...</td>\n      <td>What is your relationship with Howard Stark?</td>\n      <td>-2.953407</td>\n      <td>0.000000</td>\n      <td>0.444444</td>\n      <td>0.363636</td>\n      <td>0.571429</td>\n      <td>0.250000</td>\n      <td>0.200000</td>\n      <td>0.333333</td>\n      <td>0.444444</td>\n      <td>0.363636</td>\n      <td>0.571429</td>\n      <td>0.444444</td>\n      <td>0.363636</td>\n      <td>0.571429</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>kris left rehab .</td>\n      <td>kris just got out of rehab .</td>\n      <td>-2.264951</td>\n      <td>0.000000</td>\n      <td>0.444444</td>\n      <td>0.666667</td>\n      <td>0.333333</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.444444</td>\n      <td>0.666667</td>\n      <td>0.333333</td>\n      <td>0.444444</td>\n      <td>0.666667</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <th>204</th>\n      <td>|| ≥ 1 % of the total number of the child-resi...</td>\n      <td>|| ≥ 1 % which are offered or sold to the gene...</td>\n      <td>-2.883121</td>\n      <td>0.356886</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.352941</td>\n      <td>0.352941</td>\n      <td>0.352941</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>306</th>\n      <td>there 's something here !</td>\n      <td>there ’ s something here !</td>\n      <td>-2.263093</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>309</th>\n      <td>Road transport.</td>\n      <td>Road transport</td>\n      <td>-2.669473</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>312</th>\n      <td>Molecular biology.</td>\n      <td>Molecular biology</td>\n      <td>-2.083886</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>188</th>\n      <td>Economic and Monetary Union.</td>\n      <td>Economic and Monetary Union</td>\n      <td>-1.832057</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>258</th>\n      <td>Germany, Europe, and Russia.</td>\n      <td>Germany, Europe, and Russia</td>\n      <td>-1.551258</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>145 rows × 16 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df143.sort_values('rouge1_fmeasure', ascending=True)\n",
    "df = df[df['bartscore']>-3]\n",
    "df = df[df['bartscore']<-1.5]\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     | src                                                                                                   | target                                                                                                |\n",
      "|----:|:------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------|\n",
      "| 487 | for the movie that's being shot in 1 5 minutes?                                                       | for the movie that we 're shooting in about 1 5 minutes ?                                             |\n",
      "| 495 | adler took a moment to consider that, and nodded.                                                     | adler took a few seconds to consider that , then nodded thoughtfully .                                |\n",
      "|   0 | david schwartz was unaware of the fact that he was only narrowly avoided a permanent dent in his ego. | david schwartz was unaware of how narrowly he had escaped crippling and a permanent dent in his ego . |\n",
      "| 447 | i had no idea you were a stunt performer.                                                             | i had no idea you did stunt work .                                                                    |\n",
      "|  54 | seldon was no longer traveling around only when accompanied.                                          | seldon no longer traveled around only if accompanied .                                                |\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[110:115, :2].to_markdown())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}