# configs file for wandb.config
---
model_name:
  desc: Name of variation of OPT from huggingface to initialise
  value: "facebook/opt-125m"
load_from_checkpoint:
  desc: Checkpoint path to load the model from, if None will instantiate from default
  value: null
  #value: "training_checkpoints/soft-opt-epoch=000-val_loss=21.112.ckpt"

# training config
# since we are using streaming dataloaders, we cannot define an epoch as end of the dataset, hence we use
# num_batches_per_epoch = steps_per_epoch / batch_size
max_epochs:
  desc: Maximum number of training epochs
  value: 300
steps_per_epoch:
  desc: Number of steps per epoch
  value: 100
batch_size:
  desc: Batch size that each datamodule will output
  value: 1
val_check_interval:
  desc: Every n epochs validation loop is run
  value: 5.0

# checkpoint configs
checkpoint_every_n_epochs:
  desc: Every n epochs we save a checkpoint of the model
  value: 30
checkpoint_save_dir:
  desc: Directory to save checkpoints to
  value: "training_checkpoints/"
layers_to_optimize:
  desc: The name of the layers to optimize, and then save. regex matching will match even incomplete names, although try not the break this on purpose by matching more than 1 layer :)
  value: ["soft_embedding.learned_embedding"]

# optimizers config
optimizer_type:
  desc: Type of optimizer to use (currently only supports "Adam" or "SGD")
  value: "Adam"
optimizer_params:
  desc: Parameters for the optimizer in the form of a dictionary, same format as state_dict.
  # btw when using scientific notation write 1.0e-3 and not 1e-3 otherwise it is mistakenly parsed as string (bug on pyYAML side)
  value: {"lr": 1.0e-3}
lr_scheduler_type:
  desc: Type of learning rate scheduler to use (currently only supports "ReduceLROnPlateau")
  value: "ReduceLROnPlateau"
lr_scheduler_params:
  desc: Parameters for the learning rate scheduler in the form of a dictionary, same format as state_dict.
  value: {"mode": "min", "patience": 10}
lr_scheduler_config:
  desc: Learning rate scheduler configuration for pytorch lightning
  value: {"monitor": "train_loss"}

# learnable embedding config
embedding_n_tokens:
  desc: Number of learnable tokens to be prepended to the embedding
  value: 20
init_from_vocab:
  desc: Whether to intialise the learned embedding as a copy of the existing vocabulary so it does not have to be trained from scratch (basically no downside to having this always True)
  value: True
...